#ifndef HYBRID_RANSAC_H
#define HYBRID_RANSAC_H

#include <algorithm>
#include <cmath>
#include <cstddef>
#include <cstdint>
#include <limits>
#include <random>
#include <vector>

#include <RansacLib/hybrid_ransac.h>
#include <RansacLib/sampling.h>
#include <RansacLib/utils.h>

using namespace ransac_lib;
namespace madpose {

class ExtendedHybridLORansacOptions : public ransac_lib::HybridLORansacOptions {
 public:
  ExtendedHybridLORansacOptions()
      : non_min_sample_multiplier_(3) {}
  // We add this to do non minimal sampling in LO step in align with
  // the original definition of the LO step
  int non_min_sample_multiplier_;
};

// Our customized hybrid-RANSAC based on HybridLocallyOptimizedMSAC from RansacLib
// [LINK] https://github.com/tsattler/RansacLib/blob/master/RansacLib/hybrid_ransac.h
template <class Model, class ModelVector, class HybridSolver,
          class Sampler = HybridUniformSampling<HybridSolver> >
class HybridLOMSAC : public HybridRansacBase {
 public:
  // Estimates a model using a given solver. Notice that the solver contains
  // all data and is responsible to implement a non-minimal solver and
  // least-squares refinement. The latter two are optional, i.e., a dummy
  // implementation returning false is sufficient.
  // Returns the number of inliers.
  int EstimateModel(const ExtendedHybridLORansacOptions& options,
                    const HybridSolver& solver, Model* best_model,
                    HybridRansacStatistics* statistics) const {
    // Initializes all relevant variables.
    ResetStatistics(statistics);
    HybridRansacStatistics& stats = *statistics;

    const int kNumSolvers = solver.num_minimal_solvers();
    stats.num_iterations_per_solver.resize(kNumSolvers, 0);

    const int kNumDataTypes = solver.num_data_types();
    stats.inlier_ratios.resize(kNumDataTypes, 0.0);
    stats.inlier_indices.resize(kNumDataTypes);

    std::vector<double> prior_probabilities;
    solver.solver_probabilities(&prior_probabilities);

    std::vector<std::vector<int>> min_sample_sizes;
    solver.min_sample_sizes(&min_sample_sizes);

    std::vector<int> num_data;
    solver.num_data(&num_data);

    if (!VerifyData(min_sample_sizes, num_data, kNumSolvers, kNumDataTypes,
                    &prior_probabilities)) {
      return 0;
    }

    Sampler sampler(options.random_seed_, solver);

    uint32_t max_num_iterations =
        std::max(options.max_num_iterations_, options.min_num_iterations_);
    stats.num_iterations_per_solver.resize(kNumSolvers, 0u);
    std::vector<uint32_t> max_num_iterations_per_solver(
        kNumSolvers, std::max(options.max_num_iterations_per_solver_,
                              options.min_num_iterations_));

    const std::vector<double>& kSqrInlierThresh =
        options.squared_inlier_thresholds_;

    Model best_minimal_model;
    double best_min_model_score = std::numeric_limits<double>::max();

    std::vector<std::vector<int>> minimal_sample(kNumDataTypes);
    ModelVector estimated_models;

    std::mt19937 rng;
    rng.seed(options.random_seed_);

    // Runs random sampling.
    for (stats.num_iterations_total = 0u;
         stats.num_iterations_total < max_num_iterations;
         ++stats.num_iterations_total) {
      // As proposed by Lebeda et al., Local Optimization is not executed in
      // the first lo_starting_iterations_ iterations. We thus run LO on the
      // best model found so far once we reach this iteration.
      if (stats.num_iterations_total == options.lo_starting_iterations_ &&
          best_min_model_score < std::numeric_limits<double>::max()) {
        ++stats.number_lo_iterations;
        LocalOptimization(options, solver, stats.best_solver_type, &rng,
                          best_model, &(stats.best_model_score),
                          &(stats.best_solver_type));

        UpdateRANSACTerminationCriteria(options, solver, *best_model,
                                        statistics,
                                        &max_num_iterations_per_solver);
      }

      const int kSolverType =
          SelectMinimalSolver(solver, prior_probabilities, stats,
                              options.min_num_iterations_, &rng);

      if (kSolverType < -1) {
        // Since no solver could be selected, we stop Hybrid RANSAC here.
        break;
      }

      stats.num_iterations_per_solver[kSolverType] += 1;

      sampler.Sample(min_sample_sizes[kSolverType], &minimal_sample);

      // MinimalSolver returns the number of estimated models.
      const int kNumEstimatedModels =
          solver.MinimalSolver(minimal_sample, kSolverType, &estimated_models);

      if (kNumEstimatedModels > 0) {
        // Finds the best model among all estimated models.
        double best_local_score = std::numeric_limits<double>::max();
        int best_local_model_id = 0;
        GetBestEstimatedModelId(options, solver, estimated_models,
                                kNumEstimatedModels, kSqrInlierThresh,
                                kNumDataTypes, num_data, &best_local_score,
                                &best_local_model_id); // kSolverType);

        // Updates the best model found so far.
        if (best_local_score < best_min_model_score ||
            stats.num_iterations_total == options.lo_starting_iterations_) {
          const bool kBestMinModel = best_local_score < best_min_model_score;

          if (kBestMinModel) {
            // New best model (estimated from inliers found. Stores this model
            // and runs local optimization
            best_min_model_score = best_local_score;
            best_minimal_model = estimated_models[best_local_model_id];

            // Updates the best model.
            UpdateBestModel(best_min_model_score, best_minimal_model,
                            kSolverType, &(stats.best_model_score), best_model,
                            &(stats.best_solver_type));
          }

          const bool kRunLO =
              (stats.num_iterations_total >= options.lo_starting_iterations_ &&
               best_min_model_score < std::numeric_limits<double>::max());
          if ((!kBestMinModel) && (!kRunLO)) continue;

          // Performs local optimization. By construction, the local
          // optimization method returns the best model between all models found
          // by local optimization and the input model, i.e.,
          // score_refined_model <= best_min_model_score holds.
          if (kRunLO) {
            ++stats.number_lo_iterations;
            double score = best_min_model_score;
            LocalOptimization(options, solver, stats.best_solver_type, &rng,
                              &best_minimal_model, &score,
                              &(stats.best_solver_type));

            // Updates the best model.
            UpdateBestModel(score, best_minimal_model, kSolverType,
                            &(stats.best_model_score), best_model,
                            &(stats.best_solver_type));
          }

          // Updates the number of RANSAC iterations for each solver as well
          // as the number of inliers and inlier ratios for each data type.
          UpdateRANSACTerminationCriteria(options, solver, *best_model,
                                          statistics,
                                          &max_num_iterations_per_solver);
        } else {
        }
      }

      // Terminate if the current solver reaches its maximum number of
      // iterations.
      if (stats.num_iterations_per_solver[kSolverType] >=
          max_num_iterations_per_solver[kSolverType]) {
        break;
      }
    }

    // As proposed by Lebeda et al., Local Optimization is not executed in
    // the first lo_starting_iterations_ iterations. If LO-MSAC needs less than
    // lo_starting_iterations_ iterations, we run LO now.
    if (stats.num_iterations_total <= options.lo_starting_iterations_ &&
        stats.best_model_score < std::numeric_limits<double>::max()) {
      ++stats.number_lo_iterations;
      LocalOptimization(options, solver, stats.best_solver_type, &rng,
                        best_model, &(stats.best_model_score),
                        &(stats.best_solver_type));

      UpdateRANSACTerminationCriteria(options, solver, *best_model, statistics,
                                      &max_num_iterations_per_solver);
    }

    if (options.final_least_squares_) {
      Model refined_model = *best_model;
      solver.LeastSquares(stats.inlier_indices, stats.best_solver_type, &refined_model);

      double score = std::numeric_limits<double>::max();
      ScoreModel(options, solver, refined_model, kSqrInlierThresh,
                 kNumDataTypes, num_data, &score); // stats.best_solver_type);
      if (score < stats.best_model_score) {
        stats.best_model_score = score;
        *best_model = refined_model;

        // Updates the inlier ratios and the number of inliers. Updating the
        // number of RANSAC iterations is not necessary, but done here to avoid
        // code duplication.
        UpdateRANSACTerminationCriteria(options, solver, *best_model,
                                        statistics,
                                        &max_num_iterations_per_solver);
      }
    }

    return stats.best_num_inliers;
  }

 public:
  // Randomly selects a minimal solver. See Eq. 1 in Camposeco et al.
  int SelectMinimalSolver(const HybridSolver& solver,
                          const std::vector<double> prior_probabilities,
                          const HybridRansacStatistics& stats,
                          const uint32_t min_num_iterations,
                          std::mt19937* rng) const {
    double sum_probabilities = 0.0;
    const int kNumSolvers = static_cast<int>(prior_probabilities.size());
    std::vector<std::vector<int>> min_sample_sizes;
    solver.min_sample_sizes(&min_sample_sizes);
    std::vector<double> probabilities(kNumSolvers, 0);

    const int kNumDataTypes = solver.num_data_types();

    // There is a special case where all inlier ratios are 0. In this case, the
    // solvers should be sampled based on the priors.
    const double kSumInlierRatios = std::accumulate(
        stats.inlier_ratios.begin(), stats.inlier_ratios.end(), 0.0);

    for (int i = 0; i < kNumSolvers; ++i) {
      probabilities[i] = prior_probabilities[i];
      sum_probabilities += probabilities[i];
    }

    std::uniform_real_distribution<double> dist(0.0, sum_probabilities);

    const double kProb = dist(*rng);
    double current_prob = 0.0;
    for (int i = 0; i < kNumSolvers; ++i) {
      if (prior_probabilities[i] == 0.0) continue;

      current_prob += probabilities[i];
      if (kProb <= current_prob) return i;
    }
    return -1;
  }

  void GetBestEstimatedModelId(
      const ExtendedHybridLORansacOptions& options, const HybridSolver& solver,
      const ModelVector& models, const int num_models,
      const std::vector<double>& squared_inlier_thresholds,
      const int num_data_types, const std::vector<int> num_data,
      double* best_score, int* best_model_id, const int kSolverType = -1) const {
    *best_score = std::numeric_limits<double>::max();
    *best_model_id = 0;

    for (int m = 0; m < num_models; ++m) {
      double score = std::numeric_limits<double>::max();
      ScoreModel(options, solver, models[m], squared_inlier_thresholds,
                 num_data_types, num_data, &score); // kSolverType);

      if (score < *best_score) {
        *best_score = score;
        *best_model_id = m;
      }
    }
  }

  void ScoreModel(const ExtendedHybridLORansacOptions& options,
                  const HybridSolver& solver, const Model& model,
                  const std::vector<double>& squared_inlier_thresholds,
                  const int num_data_types, const std::vector<int> num_data,
                  double* score, const int kSolverType = -1) const {
    *score = 0.0;

    std::vector<std::vector<int>> min_sample_sizes;
    solver.min_sample_sizes(&min_sample_sizes);

    // *score = solver.EvaluateModel(model);
    for (int t = 0; t < num_data_types; ++t) {
      if (kSolverType >= 0 && min_sample_sizes[kSolverType][t] == 0) continue;
      for (int i = 0; i < num_data[t]; ++i) {
        double squared_error = solver.EvaluateModelOnPoint(model, t, i);
        *score += ComputeScore(squared_error, squared_inlier_thresholds[t]) *
                  options.data_type_weights_[t];
      }
    }
  }

  // MSAC (top-hat) scoring function.
  inline double ComputeScore(const double squared_error,
                             const double squared_error_threshold) const {
    return std::min(squared_error, squared_error_threshold);
  }

  int GetInliers(const HybridSolver& solver, const Model& model,
                 const std::vector<double>& squared_inlier_thresholds,
                 std::vector<std::vector<int>>* inliers, const int kSolverType = -1,
                 bool common = false) const {
    const int kNumDataTypes = solver.num_data_types();
    std::vector<int> num_data;
    solver.num_data(&num_data);

    std::vector<std::vector<int>> min_sample_sizes;
    solver.min_sample_sizes(&min_sample_sizes);
    int num_inliers = 0;

    if (!common) {
      for (int t = 0; t < kNumDataTypes; ++t) {
        if (kSolverType >= 0 && min_sample_sizes[kSolverType][t] == 0) continue;
        if (inliers != nullptr) {
          if (inliers->size() == 0) inliers->resize(kNumDataTypes);
          (*inliers)[t].clear();
        }
        for (int i = 0; i < num_data[t]; ++i) {
          double squared_error = solver.EvaluateModelOnPoint(model, t, i, true);
          if (squared_error < squared_inlier_thresholds[t]) {
            ++num_inliers;
            if (inliers != nullptr) (*inliers)[t].push_back(i);
          }
        }
      }
      return num_inliers;
    } 
    else {
      if (inliers != nullptr) {
        for (int t = 0; t < kNumDataTypes; ++t) {
          if (kSolverType >= 0 && min_sample_sizes[kSolverType][t] == 0) continue;
          if (inliers->size() == 0) inliers->resize(kNumDataTypes);
          (*inliers)[t].clear();
        }
      }
      for (int i = 0; i < num_data[0]; ++i) {
        bool is_common_inlier = true;
        for (int t = 0; t < kNumDataTypes; ++t) {
          double squared_error = solver.EvaluateModelOnPoint(model, t, i);
          if (squared_error >= squared_inlier_thresholds[t]) {
            is_common_inlier = false;
            break;
          }
        }
        if (is_common_inlier) {
          for (int t = 0; t < kNumDataTypes; ++t) {
            if (kSolverType >= 0 && min_sample_sizes[kSolverType][t] == 0) continue;
            ++num_inliers;
            if (inliers != nullptr) (*inliers)[t].push_back(i);
          }
        }
      }
      return num_inliers;
    }
  }

  void UpdateRANSACTerminationCriteria(
      const ExtendedHybridLORansacOptions& options, const HybridSolver& solver,
      const Model& model, HybridRansacStatistics* statistics,
      std::vector<uint32_t>* max_iterations) const {
    statistics->best_num_inliers =
        GetInliers(solver, model, options.squared_inlier_thresholds_,
                   &(statistics->inlier_indices)); // statistics->best_solver_type);

    const int kNumDataTypes = solver.num_data_types();
    std::vector<int> num_data;
    solver.num_data(&num_data);

    for (int d = 0; d < kNumDataTypes; ++d) {
      if (num_data[d] > 0) {
        statistics->inlier_ratios[d] =
            static_cast<double>(statistics->inlier_indices[d].size()) /
            static_cast<double>(num_data[d]);
      } else {
        statistics->inlier_ratios[d] = 0.0;
      }
    }

    std::vector<std::vector<int>> min_sample_sizes;
    solver.min_sample_sizes(&min_sample_sizes);
    const int kNumSolvers = solver.num_minimal_solvers();
    for (int s = 0; s < kNumSolvers; ++s) {
      (*max_iterations)[s] = utils::NumRequiredIterations(
          statistics->inlier_ratios, 1.0 - options.success_probability_,
          min_sample_sizes[s], options.min_num_iterations_,
          options.max_num_iterations_per_solver_);
    }
  }

  // See algorithms 2 and 3 in Lebeda et al.
  // The input model is overwritten with the refined model if the latter is
  // better, i.e., has a lower score.
  void LocalOptimization(const ExtendedHybridLORansacOptions& options,
                         const HybridSolver& solver, const int solver_type,
                         std::mt19937* rng, Model* best_minimal_model,
                         double* score_best_minimal_model,
                         int* best_solver_type) const {
    std::vector<int> num_data;
    solver.num_data(&num_data);
    const int kNumDataTypes = static_cast<int>(num_data.size());

    std::vector<std::vector<int>> min_sample_sizes;
    solver.min_sample_sizes(&min_sample_sizes);

    const double kThreshMult = options.threshold_multiplier_;
    std::vector<double> squared_inlier_thresholds =
        options.squared_inlier_thresholds_;
    std::vector<double> thresh_mult_updates(squared_inlier_thresholds.size());
    for (int i = 0; i < kNumDataTypes; ++i) {
      thresh_mult_updates[i] = 
          (kThreshMult - 1.0) * squared_inlier_thresholds[i] /
          static_cast<int>(options.num_lsq_iterations_ - 1);
      squared_inlier_thresholds[i] *= kThreshMult;
    }

    // Performs an initial least squares fit of the best model found by the
    // minimal solver so far and then determines the inliers to that model
    // under a (slightly) relaxed inlier threshold.
    Model m_init = *best_minimal_model;
    LeastSquaresFit(options, squared_inlier_thresholds, solver_type, solver,
                    rng, &m_init, true);

    double score = std::numeric_limits<double>::max();
    ScoreModel(options, solver, m_init, options.squared_inlier_thresholds_,
               kNumDataTypes, num_data, &score); // solver_type);
    UpdateBestModel(score, m_init, solver_type, score_best_minimal_model,
                    best_minimal_model, best_solver_type);

    std::vector<std::vector<int>> inliers_base;
    GetInliers(solver, m_init, options.squared_inlier_thresholds_,
               &inliers_base); //, solver_type, true);

    std::vector<int> inliers_base_all_type;
    int num_data_previous_types = 0;
    for (int i = 0; i < inliers_base.size(); i++) {
      for (auto &idx : inliers_base[i])
        inliers_base_all_type.push_back(idx + num_data_previous_types);
      num_data_previous_types += num_data[i];
    }

    // Determines the size of the non-miminal samples drawn in each LO step.
    std::vector<int> kNonMinSampleSizes(kNumDataTypes);
    const int kMinNonMinSampleSize = solver.non_minimal_sample_size(); // / kNumDataTypes + 1;
    int kMinSampleSize = solver.min_sample_size();
    int kNonMinSampleSize = 
        std::max(kMinNonMinSampleSize,
              std::min(kMinSampleSize * options.non_min_sample_multiplier_,
                          static_cast<int>(inliers_base_all_type.size()) / 2));

    // Performs the actual local optimization (LO).
    // Note that compared to the original definition of the LO step, no non-
    // minimal sample is drawn as a non-minimal sample over multiple data types
    // is not well-defined.
    // ***But we can do this in this case***
    for (int r = 0; r < options.num_lo_steps_; ++r) {
      std::vector<int> sample_all_type = inliers_base_all_type;
      utils::RandomShuffleAndResize(kNonMinSampleSize, rng, &inliers_base_all_type);

      std::vector<std::vector<int>> sample(kNumDataTypes);
      for (auto &idx : sample_all_type) {
        int t = 0;
        while (idx >= num_data[t]) {
          idx -= num_data[t];
          t++;
        }
        sample[t].push_back(idx);
      }

      Model m_non_min = m_init; // modified here
      if (!solver.NonMinimalSolver(sample, solver_type, &m_non_min)) continue;

      ScoreModel(options, solver, m_non_min,
                 options.squared_inlier_thresholds_, kNumDataTypes, num_data, &score); // solver_type);
      UpdateBestModel(score, m_non_min, solver_type, score_best_minimal_model,
                        best_minimal_model, best_solver_type);

      // Uncomment this line to fall back to original hybrid ransac in ransac lib
      // m_non_min = m_init;

      // Iterative least squares refinement. Note that a random subset of all
      // inliers is used.
      LeastSquaresFit(options, options.squared_inlier_thresholds_, solver_type,
                      solver, rng, &m_non_min);

      // The current threshold multiplier and its update.
      std::vector<double> cur_squared_inlier_thresholds = squared_inlier_thresholds;
      for (int i = 0; i < options.num_lsq_iterations_; ++i) {
        LeastSquaresFit(options, cur_squared_inlier_thresholds, solver_type, solver,
                        rng, &m_non_min);

        ScoreModel(options, solver, m_non_min,
                   options.squared_inlier_thresholds_, kNumDataTypes, num_data,
                   &score); // solver_type);
        UpdateBestModel(score, m_non_min, solver_type, score_best_minimal_model,
                        best_minimal_model, best_solver_type);
        for (int j = 0; j < kNumDataTypes; ++j) {
          cur_squared_inlier_thresholds[j] -= thresh_mult_updates[j];
        }
      }
    }
  }

  void LeastSquaresFit(const ExtendedHybridLORansacOptions& options,
                       const std::vector<double>& thresholds,
                       const int solver_type, const HybridSolver& solver,
                       std::mt19937* rng, Model* model, bool use_all_solver_inliers=false) const {
    std::vector<std::vector<int>> sample_sizes;
    solver.min_sample_sizes(&sample_sizes);
    std::vector<int> num_data;
    solver.num_data(&num_data);

    std::vector<std::vector<int>> inliers;
    int num_inliers = GetInliers(solver, *model, thresholds, &inliers);

    const int kNumDataTypes = solver.num_data_types();
    for (int i = 0; i < kNumDataTypes; ++i) {
      if (static_cast<int>(inliers[i].size()) < sample_sizes[solver_type][i]) {
        // The estimated pose has fewer inliers than required by the minimal
        // sample size. In this case, the least squares solution will likely be
        // very inaccurate and we thus skip the least squares fitting step.
        return;
      }

      sample_sizes[solver_type][i] *= options.min_sample_multiplicator_;
      sample_sizes[solver_type][i] = std::min(
          sample_sizes[solver_type][i], static_cast<int>(inliers[i].size()));
    }

    if (use_all_solver_inliers) {
      solver.LeastSquares(inliers, solver_type, model);
    } 
    else {
      int all_sample_size = 0;
      for (int i = 0; i < kNumDataTypes; ++i) {
        all_sample_size += sample_sizes[solver_type][i];
      }
      all_sample_size *= options.min_sample_multiplicator_;
      // Generate three random numbers that sum up to all_sample_size
      
      std::vector<int> inliers_all_type;
      int num_data_previous_types = 0;
      for (int i = 0; i < inliers.size(); i++) {
        for (auto &idx : inliers[i])
          inliers_all_type.push_back(idx + num_data_previous_types);
        num_data_previous_types += num_data[i];
      }
      utils::RandomShuffleAndResize(all_sample_size, rng, &inliers_all_type);
      std::vector<std::vector<int>> sample(kNumDataTypes);
      for (auto &idx : inliers_all_type) {
        int t = 0;
        while (idx >= num_data[t]) {
          idx -= num_data[t];
          t++;
        }
        sample[t].push_back(idx);
      }
      solver.LeastSquares(sample, solver_type, model);
    }
  }

  inline void UpdateBestModel(const double score_curr, const Model& m_curr,
                              const int solver_type, double* score_best,
                              Model* m_best, int* best_solver_type) const {
    if (score_curr < *score_best) {
      *score_best = score_curr;
      *m_best = m_curr;
      *best_solver_type = solver_type;
    }
  }

  // Determines whether enough data is available to run any of the minimal
  // solvers. Returns false otherwise. For those solvers that are not feasible
  // because not all data is available, the prior probability is set to 0.
  bool VerifyData(const std::vector<std::vector<int>>& min_sample_sizes,
                  const std::vector<int>& num_data, const int num_solvers,
                  const int num_data_types,
                  std::vector<double>* prior_probabilities) const {
    for (int i = 0; i < num_solvers; ++i) {
      for (int j = 0; j < num_data_types; ++j) {
        if (min_sample_sizes[i][j] > num_data[j] ||
            min_sample_sizes[i][j] < 0) {
          (*prior_probabilities)[i] = 0.0;
          break;
        }
      }
    }

    // No need to run HybridRANSAC if none of the solvers can be used.
    bool any_valid_solver = false;
    for (int i = 0; i < num_solvers; ++i) {
      if ((*prior_probabilities)[i] > 0.0) {
        any_valid_solver = true;
        break;
      }
    }

    if (!any_valid_solver) {
      return false;
    }

    return true;
  }
};

}  // namespace madpose

#endif  // HYBRID_RANSAC_H
